{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f30392e7",
   "metadata": {},
   "source": [
    "#### mobileNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10586e3",
   "metadata": {},
   "source": [
    "#### importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db2c87da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, RandomFlip, RandomRotation, RandomZoom\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcb0b88",
   "metadata": {},
   "source": [
    "#### 1. CONFIGURATION & HYPERPARAMETERS ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e39fac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 1. CONFIGURATION & HYPERPARAMETERS ---\n",
    "# NOTE: Assume DATASET_SPLIT/train and val exist for flow_from_directory\n",
    "train_dir = \"DATASET_SPLIT/train\"\n",
    "val_dir = \"DATASET_SPLIT/val\"\n",
    "\n",
    "IMAGE_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "MAX_EPOCHS_PHASE_1 = 5\n",
    "MAX_EPOCHS_PHASE_2 = 7 # Allow more epochs for fine-tuning\n",
    "LEARNING_RATE_PHASE_1 = 1e-4\n",
    "LEARNING_RATE_PHASE_2 = 1e-6 # CRITICAL: Much lower for Fine-Tuning\n",
    "DROPOUT_RATE = 0.4\n",
    "DENSE_UNITS = 512\n",
    "CHECKPOINT_PATH = \"MobileNetV2.h5\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594cfa61",
   "metadata": {},
   "source": [
    "#### 2. DATA GENERATORS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38142ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 46649 images belonging to 16 classes.\n",
      "Found 11683 images belonging to 16 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255, # Rescale to 0-1 range\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    shear_range=0.2,\n",
    "    brightness_range=[0.7, 1.3],\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# --- NOTE: Using a placeholder for num_classes since we can't run flow_from_directory ---\n",
    "# In your actual environment, this will be correctly inferred.\n",
    "try:\n",
    "    train_gen = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=IMAGE_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "    val_gen = val_datagen.flow_from_directory(\n",
    "        val_dir,\n",
    "        target_size=IMAGE_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "    NUM_CLASSES = train_gen.num_classes\n",
    "except Exception:\n",
    "    # Mock data for runnable code outside of a real directory structure\n",
    "    print(\"Using Mock Data Generator placeholders...\")\n",
    "    NUM_CLASSES = 15 # Placeholder\n",
    "    \n",
    "    # --- Mock Data to make the script runnable ---\n",
    "    X_train_mock = np.random.rand(400, 224, 224, 3).astype('float32')\n",
    "    y_train_mock = tf.keras.utils.to_categorical(np.random.randint(0, NUM_CLASSES, 400), num_classes=NUM_CLASSES)\n",
    "    X_val_mock = np.random.rand(100, 224, 224, 3).astype('float32')\n",
    "    y_val_mock = tf.keras.utils.to_categorical(np.random.randint(0, NUM_CLASSES, 100), num_classes=NUM_CLASSES)\n",
    "\n",
    "    train_gen = (X_train_mock, y_train_mock)\n",
    "    val_gen = (X_val_mock, y_val_mock)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a825d9ea",
   "metadata": {},
   "source": [
    "#### 3. BUILD THE TRANSFER LEARNING MODEL FUNCTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a84ff40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_classes, dropout_rate, dense_units):\n",
    "    base_model = MobileNetV2(\n",
    "        input_shape=(224, 224, 3),\n",
    "        include_top=False,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "    base_model.trainable = False  # Freeze base for initial training\n",
    "\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(dense_units, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(num_classes, activation='softmax', name=\"predictions_head\")\n",
    "    ])\n",
    "    return model, base_model\n",
    "\n",
    "model, base_model = build_model(NUM_CLASSES, DROPOUT_RATE, DENSE_UNITS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36576555",
   "metadata": {},
   "source": [
    "##### 4.CALLBACKS ---\n",
    "#### Adjusted patience for stability in Phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "527501cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_p1 = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=8, # Increased patience for stability\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    CHECKPOINT_PATH,\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callbacks_p1 = [early_stop_p1, reduce_lr, checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e39e55",
   "metadata": {},
   "source": [
    "#### 5. PHASE 1: TRAIN THE CLASSIFICATION HEAD (Feature Extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7268aa4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PHASE 1: Training the New Classification Head ---\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- PHASE 1: Training the New Classification Head ---\")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE_PHASE_1),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Use fit() for mock data, fit_generator() or fit() for actual generators\n",
    "if isinstance(train_gen, tuple):\n",
    "    history = model.fit(\n",
    "        train_gen[0], train_gen[1],\n",
    "        validation_data=val_gen,\n",
    "        epochs=MAX_EPOCHS_PHASE_1,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=callbacks_p1,\n",
    "        verbose=2\n",
    "    )\n",
    "else:\n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        validation_data=val_gen,\n",
    "        epochs=MAX_EPOCHS_PHASE_1,\n",
    "        callbacks=callbacks_p1,\n",
    "        verbose=2\n",
    "    )\n",
    "print(\"Phase 1 complete. Best weights saved to checkpoint.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e58ba70",
   "metadata": {},
   "source": [
    "##### 6. PHASE 2: FINE-TUNING FOR PEAK ACCURACY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989d69e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n--- PHASE 2: Fine-Tuning the Base Model ---\")\n",
    "\n",
    "# Load the best weights saved from Phase 1\n",
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    model.load_weights(CHECKPOINT_PATH)\n",
    "    print(\"Loaded best weights from Phase 1.\")\n",
    "else:\n",
    "    print(\"Checkpoint not found (likely mock training), proceeding without loading.\")\n",
    "\n",
    "# 6.1 Strategic Unfreezing (Unfreeze the top ~20% of the layers)\n",
    "base_model.trainable = True\n",
    "\n",
    "# We keep the first 80% of layers frozen to protect the low-level feature detector\n",
    "# Total layers in EfficientNetB0 is typically 238\n",
    "num_unfreeze_layers = int(len(base_model.layers) * 0.20)\n",
    "for layer in base_model.layers[:-num_unfreeze_layers]:\n",
    "    layer.trainable = False\n",
    "\n",
    "print(f\"Unfrozen the last {num_unfreeze_layers} layers of the base model for specialization.\")\n",
    "# You can see the total trainable parameters with model.summary()\n",
    "\n",
    "# 6.2 Re-compile with extremely low learning rate\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE_PHASE_2), # Use 1e-6\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# New Callbacks list for Phase 2: higher patience for slow fine-tuning\n",
    "callbacks_p2 = [\n",
    "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),\n",
    "    ModelCheckpoint(CHECKPOINT_PATH, monitor='val_accuracy', save_best_only=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=5, min_lr=1e-8, verbose=1) # Adjusted LR factor/patience\n",
    "]\n",
    "\n",
    "print(\"Starting Phase 2 Fine-Tuning...\")\n",
    "\n",
    "if isinstance(train_gen, tuple):\n",
    "    history_fine_tune = model.fit(\n",
    "        train_gen[0], train_gen[1],\n",
    "        validation_data=val_gen,\n",
    "        epochs=MAX_EPOCHS_PHASE_2,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=callbacks_p2,\n",
    "        verbose=2\n",
    "    )\n",
    "else:\n",
    "    history_fine_tune = model.fit(\n",
    "        train_gen,\n",
    "        validation_data=val_gen,\n",
    "        epochs=MAX_EPOCHS_PHASE_2,\n",
    "        callbacks=callbacks_p2,\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "print(\"\\nTraining complete. The highest validation accuracy is saved in:\", CHECKPOINT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
